import os
# âœ… åœ¨ç¨‹å¼æœ€é–‹é ­æ–°å¢ FFmpeg å„ªåŒ–è¨­å®š
os.environ["OPENCV_FFMPEG_CAPTURE_OPTIONS"] = "rtsp_transport;tcp|buffer_size;1048576|max_delay;500000"
os.environ["FFMPEG_LOG_LEVEL"] = "warning"  # é™ä½ FFmpeg æ—¥èªŒç­‰ç´š

import cv2
import time
import threading
import numpy as np
import os
import logging
import requests
from datetime import datetime
from queue import Queue
from ultralytics import YOLO
import mediapipe as mp
print("âœ… mediapipe import success")
import torch
import subprocess as sp
import math
import json
import uuid  
import av 
print(f"âœ… PyAV Version: {av.__version__}")
print("âœ… Linked FFmpeg Libraries:")
for k, v in av.library_versions.items():
    print(f"  - {k}: {v}")

import traceback
import sys

def resource_path(relative_path):
    """æ‰“åŒ…æˆ .exe æ™‚å–å¾—è³‡æºå¯¦é«”è·¯å¾‘"""
    if hasattr(sys, '_MEIPASS'):
        return os.path.join(sys._MEIPASS, relative_path)
    return os.path.join(os.path.abspath("."), relative_path)

# å–å¾—ç›®å‰è…³æœ¬æ‰€åœ¨è³‡æ–™å¤¾
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# âœ… æ¨¡å‹è·¯å¾‘ï¼ˆæ‰“åŒ…å¾Œæ”¯æ´ PyInstallerï¼‰
pose_model_path = resource_path("model/yolo11n-pose.pt")
maskcap_model_path = resource_path("model/mask_cap/best.pt")
face_model_path = resource_path("model/face_landmarker.task")
pose_task_path = resource_path("model/pose_landmarker_full.task")

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
if torch.cuda.is_available():
    print(f"GPU count: {torch.cuda.device_count()}")
    print(f"Current GPU: {torch.cuda.get_device_name(0)}")

# === ğŸ” å¤šåŸ·è¡Œç·’ä¿è­·ç”¨ Lockï¼ˆæ–°å¢ï¼‰ ===
logfile_lock = threading.Lock()
alert_times_lock = threading.Lock()

# æ¯ç¨®é¡åˆ¥ï¼ˆçœ¼ç›é–‰åˆã€è½‰é ­ï¼‰çš„å†·å»æ™‚é–“ï¼ˆç§’ï¼‰
alert_cooldowns = {
    "EYES CLOSED": 300,
    "HEAD TURNED": 300,
    "MISSING CAP": 600,
    "MISSING MASK": 600,
}

# æ¯æ”¯æ”å½±æ©Ÿçš„ä¸Šæ¬¡è§¸ç™¼æ™‚é–“è¨˜éŒ„
last_alert_times = {}

# === è£ç½®é‹ç®—æ¨¡å¼è¨­å®š ===
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"âœ… ä½¿ç”¨è¨­å‚™: {DEVICE}")  

# è¨­å®š log æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# === è®€å–æ”å½±æ©Ÿè¨­å®šæª”æ¡ˆ ===
def load_cameras_config(config_file="cameras_config.txt"):
    """
    å¾å¤–éƒ¨æª”æ¡ˆè®€å–æ”å½±æ©Ÿè¨­å®š
    æ”¯æ´JSONæ ¼å¼çš„è¨­å®šæª”
    """
    try:
        if not os.path.exists(config_file):
            # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹é è¨­è¨­å®šæª”
            create_default_config(config_file)
            
        with open(config_file, 'r', encoding='utf-8') as f:
            cameras_config = json.load(f)
            
        logging.info(f"âœ… æˆåŠŸè®€å–è¨­å®šæª”: {config_file}, å…± {len(cameras_config)} å°æ”å½±æ©Ÿ")
        return cameras_config
        
    except json.JSONDecodeError as e:
        logging.error(f"âŒ è¨­å®šæª”æ ¼å¼éŒ¯èª¤: {e}")
        logging.info("ğŸ’¡ è«‹æª¢æŸ¥ cameras_config.txt æ˜¯å¦ç‚ºæœ‰æ•ˆçš„JSONæ ¼å¼")
        return []
    except Exception as e:
        logging.error(f"âŒ è®€å–è¨­å®šæª”å¤±æ•—: {e}")
        return []

def create_default_config(config_file="cameras_config.txt"):
    """
    å»ºç«‹é è¨­çš„æ”å½±æ©Ÿè¨­å®šæª”æ¡ˆ
    """
    default_config = [
        {
            "rtsp_url": "rtsp://hikvision:Unitech0815!@10.20.233.40/Streaming/Channels/101",
            "cooldown_seconds": 300,
            "eye_close_threshold": 0.015,
            "close_threshold_frames": 30,
            "head_turn_threshold": 50,
            "head_turn_frames": 100,
            "missing_cap_frames": 50,
            "missing_mask_frames": 50,
            "camera_id": "202001",
            "location": "UT2-2F-01",
            "upload_url": "https://eip.pcbut.com.tw/File/UploadYoloImage"
        }
    ]
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=4)
        logging.info(f"âœ… å·²å»ºç«‹é è¨­è¨­å®šæª”: {config_file}")
    except Exception as e:
        logging.error(f"âŒ å»ºç«‹é è¨­è¨­å®šæª”å¤±æ•—: {e}")

# è¼‰å…¥æ”å½±æ©Ÿè¨­å®š
cameras_config = load_cameras_config()

# å¦‚æœæ²’æœ‰æˆåŠŸè¼‰å…¥è¨­å®šï¼Œç¨‹å¼çµæŸ
if not cameras_config:
    logging.error("âŒ ç„¡æ³•è¼‰å…¥æ”å½±æ©Ÿè¨­å®šï¼Œç¨‹å¼çµæŸ")
    exit(1)

# âœ… æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆåŸ·è¡Œéšæ®µï¼‰
for path in [pose_model_path, maskcap_model_path, face_model_path, pose_task_path]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆï¼š{path}")
    
# âœ… åˆå§‹åŒ– pose_modelï¼Œä¸¦é—œé–‰ fuse é˜²æ­¢ bn éŒ¯èª¤
pose_model = YOLO(pose_model_path).to(DEVICE)
pose_model.model.fuse = lambda *args, **kwargs: pose_model.model  # ç¦ç”¨ fuse
pose_model.predict(np.zeros((480, 640, 3), dtype=np.uint8))  # é å…ˆè§¸ç™¼åˆå§‹åŒ–ï¼ˆé»‘åœ–ï¼‰
logging.info("âœ… Pose æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆå« fuse disabledï¼‰")

# âœ… åˆå§‹åŒ– maskcap_modelï¼ˆåŒæ¨£ä¿éšªèµ·è¦‹ä¹Ÿé—œé–‰ fuseï¼‰
maskcap_model = YOLO(maskcap_model_path).to(DEVICE)
maskcap_model.model.fuse = lambda *args, **kwargs: maskcap_model.model
maskcap_model.predict(np.zeros((480, 640, 3), dtype=np.uint8))  # é æ¨è«–ä¸€æ¬¡
logging.info("âœ… Mask+Cap æ¨¡å‹å·²åˆå§‹åŒ–")

upload_queue = Queue(maxsize=1000)

# â¬…ï¸ å…¨åŸŸè®Šæ•¸ï¼Œè¨˜éŒ„å‰ä¸€æ¬¡ç‹€æ…‹
last_detection_status = None

def is_within_time_period():
    global last_detection_status
    current_time = datetime.now().time()
    periods = [
        ("00:30", "02:30"), ("02:40", "03:30"), ("04:00", "06:00"), ("06:10", "08:00"),
        ("08:30", "09:50"), ("10:00", "11:40"), ("12:10", "14:30"), ("14:40", "16:00"),
        ("16:30", "18:30"), ("19:00", "21:00"), ("21:10", "22:50"), ("23:00", "00:00"),
    ]

    now_status = False
    matched_period = None
    for start_str, end_str in periods:
        start = datetime.strptime(start_str, "%H:%M").time()
        end = datetime.strptime(end_str, "%H:%M").time()
        if start <= current_time <= end:
            now_status = True
            matched_period = f"{start_str}~{end_str}"
            break

    if last_detection_status != now_status:
        if now_status:
            logging.info(f"[æ™‚æ®µæª¢æŸ¥] âœ… ç›®å‰æ™‚é–“ {current_time.strftime('%H:%M:%S')} å±¬æ–¼åµæ¸¬å•Ÿç”¨æ™‚æ®µï¼ˆ{matched_period}ï¼‰")
        else:
            logging.info(f"[æ™‚æ®µæª¢æŸ¥] â¸ï¸ ç›®å‰æ™‚é–“ {current_time.strftime('%H:%M:%S')} ä¸åœ¨åµæ¸¬æ™‚æ®µï¼Œæš«åœè¾¨è­˜")
        last_detection_status = now_status  # æ›´æ–°ç‹€æ…‹

    return now_status

# === æ–°å¢ï¼šå–å¾—ç›®å‰ç‹€æ…‹çš„å‡½æ•¸ ===
def get_current_status():
    """å–å¾—ç›®å‰æ˜¯WORKé‚„æ˜¯SKIPç‹€æ…‹"""
    return "WORK" if is_within_time_period() else "SKIP"

# === æ–°å¢ï¼šè¨ˆç®—é ­éƒ¨è½‰å‘è§’åº¦çš„å‡½æ•¸ ===
def calculate_head_angle(keypoints, frame_width=640, frame_height=480, confidence_threshold=0.5):
    """
    ä½¿ç”¨YOLO poseé—œéµé»è¨ˆç®—é ­éƒ¨è½‰å‘è§’åº¦ï¼ŒåŠ å…¥ç•«é¢ä½ç½®è£œå„Ÿ
    YOLO poseé—œéµé»ç´¢å¼•:
    0: é¼»å°–, 1: å·¦çœ¼, 2: å³çœ¼, 3: å·¦è€³, 4: å³è€³
    """
    try:
        # æª¢æŸ¥é—œéµé»ç½®ä¿¡åº¦
        if len(keypoints) < 5:
            return None, None, None
        
        # å–å¾—é ­éƒ¨é—œéµé»åº§æ¨™ (x, y, confidence)
        nose = keypoints[0][:2] if keypoints[0][2] > confidence_threshold else None
        left_eye = keypoints[1][:2] if keypoints[1][2] > confidence_threshold else None
        right_eye = keypoints[2][:2] if keypoints[2][2] > confidence_threshold else None
        left_ear = keypoints[3][:2] if keypoints[3][2] > confidence_threshold else None
        right_ear = keypoints[4][:2] if keypoints[4][2] > confidence_threshold else None
        
        # è‡³å°‘éœ€è¦é¼»å­å’Œé›™çœ¼
        if nose is None or left_eye is None or right_eye is None:
            return None, None, None
            
        # è¨ˆç®—é ­éƒ¨ä¸­å¿ƒé»
        head_center_x = (left_eye[0] + right_eye[0]) / 2
        head_center_y = (left_eye[1] + right_eye[1]) / 2
        
        # è¨ˆç®—äººç‰©åœ¨ç•«é¢ä¸­çš„ä½ç½®åç§»
        center_offset_x = head_center_x - frame_width / 2
        center_offset_ratio = center_offset_x / (frame_width / 2)  # -1åˆ°1ä¹‹é–“
        
        # æ–¹æ³•1ï¼šä½¿ç”¨çœ¼ç›è¨ˆç®—æ°´å¹³è½‰å‘è§’åº¦ï¼ˆåŠ å…¥ä½ç½®è£œå„Ÿï¼‰
        horizontal_angle = None
        if left_eye is not None and right_eye is not None:
            # è¨ˆç®—é›™çœ¼é€£ç·šèˆ‡æ°´å¹³ç·šçš„å¤¾è§’
            eye_vector = np.array(right_eye) - np.array(left_eye)
            raw_angle = math.degrees(math.atan2(eye_vector[1], eye_vector[0]))
            
            # ä½ç½®è£œå„Ÿï¼šç•¶äººç‰©åé›¢ä¸­å¿ƒæ™‚ï¼Œèª¿æ•´è§’åº¦åˆ¤æ–·
            position_compensation = center_offset_ratio * 15  # æœ€å¤§è£œå„Ÿ15åº¦
            horizontal_angle = raw_angle - position_compensation
            
        # æ–¹æ³•2ï¼šä½¿ç”¨çœ¼ç›é–“è·å’Œé¼»å­ä½ç½®åˆ¤æ–·å´é¢è½‰å‘
        face_asymmetry = None
        if left_eye is not None and right_eye is not None and nose is not None:
            # è¨ˆç®—é¼»å­åˆ°é›™çœ¼çš„è·é›¢æ¯”ä¾‹
            left_eye_to_nose = np.linalg.norm(np.array(nose) - np.array(left_eye))
            right_eye_to_nose = np.linalg.norm(np.array(nose) - np.array(right_eye))
            
            if left_eye_to_nose + right_eye_to_nose > 0:
                # æ­£é¢æ™‚æ‡‰è©²æ¥è¿‘0ï¼Œå´é¢æ™‚æœƒæœ‰æ˜é¡¯å·®ç•°
                face_asymmetry = (right_eye_to_nose - left_eye_to_nose) / (left_eye_to_nose + right_eye_to_nose)
        
        # æ–¹æ³•3ï¼šä½¿ç”¨è€³æœµå’Œé¼»å­è¨ˆç®—å´é¢è½‰å‘ç¨‹åº¦ï¼ˆä½œç‚ºè¼”åŠ©ï¼‰
        side_turn_ratio = None
        if left_ear is not None and right_ear is not None and nose is not None:
            nose_x = nose[0]
            left_ear_x = left_ear[0]
            right_ear_x = right_ear[0]
            
            # è¨ˆç®—é¼»å­ç›¸å°æ–¼é›™è€³ä¸­é»çš„åç§»æ¯”ä¾‹
            ear_center_x = (left_ear_x + right_ear_x) / 2
            ear_width = abs(right_ear_x - left_ear_x)
            
            if ear_width > 0:
                side_turn_ratio = (nose_x - ear_center_x) / ear_width
        
        return horizontal_angle, face_asymmetry, side_turn_ratio
        
    except Exception as e:
        logging.error(f"è¨ˆç®—é ­éƒ¨è§’åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None, None

def is_head_turned(horizontal_angle, face_asymmetry, side_turn_ratio, 
                  threshold_angle=25, asymmetry_threshold=0.15, side_threshold=0.4):
    """
    åˆ¤æ–·æ˜¯å¦è½‰é ­ï¼ˆä½¿ç”¨å¤šé‡åˆ¤æ–·æ¨™æº–ï¼‰
    threshold_angle: æ°´å¹³è§’åº¦é–¾å€¼ï¼ˆåº¦ï¼‰- é™ä½ä»¥æ¸›å°‘èª¤åˆ¤
    asymmetry_threshold: è‡‰éƒ¨ä¸å°ç¨±é–¾å€¼
    side_threshold: å´é¢è½‰å‘æ¯”ä¾‹é–¾å€¼
    """
    turned_indicators = 0
    total_indicators = 0
    
    # æª¢æŸ¥æ°´å¹³è§’åº¦ï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰
    if horizontal_angle is not None:
        total_indicators += 1
        if abs(horizontal_angle) > threshold_angle:
            turned_indicators += 1
    
    # æª¢æŸ¥è‡‰éƒ¨ä¸å°ç¨±åº¦ï¼ˆæ›´å¯é çš„æŒ‡æ¨™ï¼‰
    if face_asymmetry is not None:
        total_indicators += 1
        if abs(face_asymmetry) > asymmetry_threshold:
            turned_indicators += 1
    
    # æª¢æŸ¥å´é¢è½‰å‘æ¯”ä¾‹ï¼ˆè¼”åŠ©æŒ‡æ¨™ï¼‰
    if side_turn_ratio is not None:
        total_indicators += 1
        if abs(side_turn_ratio) > side_threshold:
            turned_indicators += 1
    
    # è‡³å°‘éœ€è¦2å€‹æŒ‡æ¨™éƒ½åˆ¤æ–·ç‚ºè½‰é ­æ‰èªå®šç‚ºè½‰é ­
    if total_indicators >= 2:
        return turned_indicators >= 2
    elif total_indicators == 1:
        # åªæœ‰ä¸€å€‹æŒ‡æ¨™æ™‚ï¼Œéœ€è¦æ›´åš´æ ¼çš„æ¨™æº–
        if horizontal_angle is not None and abs(horizontal_angle) > 35:
            return True
        if face_asymmetry is not None and abs(face_asymmetry) > 0.25:
            return True
            
    return False

class AlertCooldownManager:
    def __init__(self):
        self.cooldowns = alert_cooldowns.copy()
        self.last_times = {}

    def is_in_cooldown(self, camera_id, alert_type):
        key = f"{camera_id}_{alert_type}"
        now = time.time()
        last = self.last_times.get(key, 0)
        duration = self.cooldowns.get(alert_type, 300)
        return now - last < duration

    def update_last_time(self, camera_id, alert_type):
        key = f"{camera_id}_{alert_type}"
        self.last_times[key] = time.time()

    def get_remaining_time(self, camera_id, alert_type):
        key = f"{camera_id}_{alert_type}"
        last = self.last_times.get(key, 0)
        duration = self.cooldowns.get(alert_type, 300)
        return max(0, duration - (time.time() - last))
    
alert_cooldown_mgr = AlertCooldownManager()

class CameraMonitor:
    def __init__(self, config, index):
        self.config = config
        self.camera_index = index
        self.frame = np.ones((480, 640, 3), dtype=np.uint8)
        self.last_notification_time = 0
        self.eye_close_counter = 0
        self.head_turn_counter = 0
        self.l_eye_value = 0.0
        self.r_eye_value = 0.0
        self.head_angle = 0.0
        self.face_asymmetry = 0.0
        self.side_turn_ratio = 0.0
        self.running = True
        self.maskcap_model = maskcap_model
        self.missing_cap_count = 0
        self.missing_mask_count = 0
        
        # âœ… æ–°å¢ï¼šæ™ºèƒ½è®€å–æ¨¡å¼é¸æ“‡
        self.use_pyav = True  # é è¨­ä½¿ç”¨ PyAV
        self.pyav_error_count = 0  # PyAV éŒ¯èª¤è¨ˆæ•¸
        self.max_pyav_errors = 50  # PyAV éŒ¯èª¤ä¸Šé™ï¼Œè¶…éå‰‡åˆ‡æ› OpenCV
        
        # åˆå§‹åŒ– MediaPipe FaceMesh
        mp_face_mesh = mp.solutions.face_mesh
        self.face_detector = mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
    def reset_counter(self, alert_type):
        if alert_type == "EYES CLOSED":
            self.eye_close_counter = 0
        elif alert_type == "HEAD TURNED":
            self.head_turn_counter = 0
        elif alert_type == "MISSING CAP":
            self.missing_cap_count = 0
        elif alert_type == "MISSING MASK":
            self.missing_mask_count = 0

    def read_thread_func(self):
        """æ™ºèƒ½é¸æ“‡è®€å–æ–¹æ³•"""
        if self.use_pyav:
            try:
                self.read_with_pyav()
            except Exception as e:
                logging.error(f"âŒ {self.config['location']} PyAV å®Œå…¨å¤±æ•—ï¼Œåˆ‡æ›åˆ° OpenCV: {e}")
                self.use_pyav = False
                self.read_with_opencv()
        else:
            self.read_with_opencv()

    def read_with_pyav(self):
        """ä½¿ç”¨å„ªåŒ–çš„ PyAV è®€å–ï¼ˆå«éŒ¯èª¤ç›£æ§ï¼‰"""
        retry_count = 0
        max_retry = 20

        while self.running and retry_count < max_retry:
            try:
                logging.info(f"ğŸ“¡ [PyAV] é€£æ¥æ”å½±æ©Ÿ {self.config['camera_id']} ({self.config['location']})")
                
                # âœ… å„ªåŒ–å¾Œçš„ PyAV è¨­å®š - ç§»é™¤ç¡¬é«”åŠ é€Ÿé¿å… MB éŒ¯èª¤
                container = av.open(self.config["rtsp_url"], options={
                    "rtsp_transport": "tcp",
                    "buffer_size": "1048576",        # 1MB ç·©è¡å€
                    "max_delay": "500000",           # æœ€å¤§å»¶é² 500ms
                    "stimeout": "5000000",           # é€£ç·šè¶…æ™‚ 5s
                    "fflags": "nobuffer+fastseek+flush_packets",  # é™ä½å»¶é²
                    "flags": "low_delay",            # ä½å»¶é²æ¨¡å¼
                    "analyzeduration": "1000000",    # åˆ†ææ™‚é•· 1s
                    "probesize": "1048576",         # æ¢æ¸¬å¤§å° 1MB
                    "threads": "2"                   # å¢åŠ è§£ç¢¼ç·šç¨‹
                    # âŒ ç§»é™¤ç¡¬é«”åŠ é€Ÿé¿å… MB è§£ç¢¼éŒ¯èª¤
                    # "hwaccel": "cuda",
                    # "hwaccel_output_format": "cuda",
                })
                
                retry_count = 0  # æˆåŠŸé–‹å•Ÿå¾Œé‡ç½®è¨ˆæ•¸
                frame_count = 0
                consecutive_decode_errors = 0
                max_consecutive_errors = 20  # é€£çºŒéŒ¯èª¤ä¸Šé™
                
                for frame in container.decode(video=0):
                    if not self.running:
                        break
                        
                    try:
                        img = frame.to_ndarray(format="bgr24")
                        self.frame = cv2.resize(img, (640, 480))
                        frame_count += 1
                        consecutive_decode_errors = 0  # æˆåŠŸå‰‡é‡ç½®éŒ¯èª¤è¨ˆæ•¸
                        
                    except Exception as decode_err:
                        consecutive_decode_errors += 1
                        self.pyav_error_count += 1
                        
                        # åªè¨˜éŒ„å‰å¹¾æ¬¡éŒ¯èª¤ï¼Œé¿å…æ—¥èªŒæ´ªæ°´
                        if consecutive_decode_errors <= 3:
                            logging.warning(f"âš ï¸ {self.config['location']} è§£ç¢¼å¤±æ•— ({consecutive_decode_errors}): {decode_err}")
                        
                        # âŒ éŒ¯èª¤éå¤šæ™‚åˆ‡æ›åˆ° OpenCV
                        if self.pyav_error_count >= self.max_pyav_errors:
                            logging.warning(f"ğŸ”„ {self.config['location']} PyAV éŒ¯èª¤ç´¯è¨ˆéå¤š ({self.pyav_error_count})ï¼Œåˆ‡æ›åˆ° OpenCV")
                            self.use_pyav = False
                            container.close()
                            return self.read_with_opencv()
                        
                        # é€£çºŒéŒ¯èª¤éå¤šæ™‚é‡æ–°é€£ç·š
                        if consecutive_decode_errors >= max_consecutive_errors:
                            logging.error(f"ğŸ’¥ {self.config['location']} é€£çºŒè§£ç¢¼éŒ¯èª¤éå¤šï¼Œé‡æ–°é€£ç·š")
                            break
                            
                        continue

                    # âœ… æ§åˆ¶å¹€ç‡ï¼Œé¿å…éåº¦è™•ç†
                    time.sleep(0.033)  # ç´„30 FPS

                container.close()
                logging.info(f"ğŸ“´ [PyAV] é¡é ­ {self.config['camera_id']} çµæŸä¸²æµ (å…±è™•ç† {frame_count} å¹€)")

            except Exception as e:
                retry_count += 1
                logging.error(f"âŒ {self.config['location']} PyAV æ¥æ”¶éŒ¯èª¤ (å˜—è©¦ {retry_count}): {e}")
                
                # âœ… æ ¹æ“šéŒ¯èª¤é¡å‹èª¿æ•´é‡è©¦ç­–ç•¥
                if "timeout" in str(e).lower() or "connection" in str(e).lower():
                    retry_interval = 3  # ç¶²è·¯å•é¡Œå¿«é€Ÿé‡è©¦
                else:
                    retry_interval = 8  # å…¶ä»–å•é¡Œå»¶é•·é–“éš”
                
                time.sleep(retry_interval)

        # PyAV å®Œå…¨å¤±æ•—ï¼Œåˆ‡æ› OpenCV
        if retry_count >= max_retry:
            logging.warning(f"ğŸ”„ {self.config['location']} PyAV é‡è©¦æ¬¡æ•¸è€—ç›¡ï¼Œæ°¸ä¹…åˆ‡æ›åˆ° OpenCV")
            self.use_pyav = False
            self.read_with_opencv()

    def read_with_opencv(self):
        """ä½¿ç”¨ OpenCV å‚™ç”¨è®€å–æ–¹æ³•"""
        retry_count = 0
        max_retry = 9999
        
        while self.running:
            cap = None
            try:
                logging.info(f"ğŸ“¡ [OpenCVå‚™ç”¨] é€£æ¥æ”å½±æ©Ÿ {self.config['camera_id']} ({self.config['location']})")
                
                # âœ… OpenCV å„ªåŒ–è¨­å®š
                cap = cv2.VideoCapture(self.config["rtsp_url"], cv2.CAP_FFMPEG)
                
                # è¨­å®šç·©è¡å€å¤§å°å’Œå¹€ç‡
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                cap.set(cv2.CAP_PROP_FPS, 25)
                
                # æª¢æŸ¥æ˜¯å¦æˆåŠŸé–‹å•Ÿ
                if not cap.isOpened():
                    raise Exception("OpenCV ç„¡æ³•é–‹å•Ÿ RTSP ä¸²æµ")
                    
                retry_count = 0
                frame_count = 0
                consecutive_fails = 0
                max_consecutive_fails = 15

                while self.running:
                    ret, frame = cap.read()
                    
                    if not ret:
                        consecutive_fails += 1
                        if consecutive_fails >= max_consecutive_fails:
                            logging.warning(f"âš ï¸ {self.config['location']} é€£çºŒè®€å–å¤±æ•— {consecutive_fails} æ¬¡ï¼Œé‡æ–°é€£ç·š")
                            break
                        time.sleep(0.1)
                        continue
                        
                    consecutive_fails = 0
                    frame_count += 1
                    
                    # èª¿æ•´ç•«é¢å¤§å°
                    self.frame = cv2.resize(frame, (640, 480))
                    
                    # æ§åˆ¶å¹€ç‡
                    time.sleep(0.04)  # ç´„25 FPS

                logging.info(f"ğŸ“´ [OpenCVå‚™ç”¨] çµæŸä¸²æµ (å…±è™•ç† {frame_count} å¹€)")

            except Exception as e:
                retry_count += 1
                logging.error(f"âŒ {self.config['location']} OpenCV æ¥æ”¶éŒ¯èª¤ (å˜—è©¦ {retry_count}): {e}")
                
            finally:
                if cap:
                    cap.release()

            if retry_count >= max_retry:
                logging.critical(f"â›” {self.config['location']} OpenCV ä¹Ÿé”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                break

            retry_interval = 5
            logging.info(f"ğŸ”„ {self.config['location']} OpenCVå‚™ç”¨æ–¹æ¡ˆ {retry_interval}ç§’å¾Œé‡è©¦")
            time.sleep(retry_interval)

    def process_thread_func(self):
        while self.running:
            frame = self.frame.copy()

            # âœ… åˆå§‹åŒ–æ¯å¹€çš„æª¢æ¸¬ç‹€æ…‹
            cap_detected = False
            mask_detected = False
            mouth_detected = False

            if not is_within_time_period():
                time.sleep(1)
                continue
            try:
                # === æ­¥é©Ÿ 1ï¼šä½¿ç”¨ MediaPipe ç¢ºèªæœ‰è‡‰ ===
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                face_results = self.face_detector.process(rgb)
                if not face_results.multi_face_landmarks:
                    self.head_turn_counter = 0
                    self.eye_close_counter = 0
                    self.missing_cap_count = 0
                    self.missing_mask_count = 0
                    time.sleep(0.1)
                    continue

                # === æ­¥é©Ÿ 2ï¼šYOLO Pose å–å¾—ä¸»é«” keypoints ===
                results = pose_model(frame, verbose=False)
                main_person = None
                min_pose_confidence = 0.90
                for r in results:
                    if r.keypoints is not None:
                        for kp in r.keypoints.data:
                            keypoints = kp.cpu().numpy()
                            high_conf_pts = [pt for pt in keypoints if pt[2] >= min_pose_confidence]
                            if len(high_conf_pts) >= 5:
                                main_person = keypoints
                                break
                    if main_person is not None:
                        break

                if main_person is None:
                    self.head_turn_counter = 0
                    self.eye_close_counter = 0
                    self.missing_cap_count = 0
                    self.missing_mask_count = 0
                    time.sleep(0.1)
                    continue

                # === é ­éƒ¨è§’åº¦åˆ¤æ–· ===
                horizontal_angle, face_asymmetry, side_turn_ratio = calculate_head_angle(main_person, 640, 480)
                self.head_angle = horizontal_angle or 0.0
                self.face_asymmetry = face_asymmetry or 0.0
                self.side_turn_ratio = side_turn_ratio or 0.0
                head_turned_current_frame = is_head_turned(horizontal_angle, face_asymmetry, side_turn_ratio)
                if head_turned_current_frame:
                    if not alert_cooldown_mgr.is_in_cooldown(self.config["camera_id"], "HEAD TURNED"):
                        self.head_turn_counter += 1
                    else:
                        logging.debug("ğŸ‘€ HEAD TURNED å†·å»ä¸­ï¼Œè·³éç´¯è¨ˆ")
                else:
                    self.head_turn_counter = 0

                eyes_closed_current_frame = False
                min_valid_eye_open = 0.005  # æœ€å°æœ‰æ•ˆè·é›¢ï¼šå°æ–¼æ­¤å€¼è¦–ç‚ºé®æ“‹æˆ–åµæ¸¬ç•°å¸¸

                for lm in face_results.multi_face_landmarks:
                    l_eye = abs(lm.landmark[145].y - lm.landmark[159].y)
                    r_eye = abs(lm.landmark[374].y - lm.landmark[386].y)
                    self.l_eye_value = l_eye
                    self.r_eye_value = r_eye

                    # âœ… åƒ…ç•¶é›™çœ¼çš†ä½æ–¼è¨­å®šé–€æª»ï¼Œä¸”å¤§æ–¼æœ€å°åµæ¸¬å€¼ï¼Œæ‰è¦–ç‚ºé–‰çœ¼
                    if (
                        min_valid_eye_open < l_eye < self.config["eye_close_threshold"] and
                        min_valid_eye_open < r_eye < self.config["eye_close_threshold"]
                    ):
                        eyes_closed_current_frame = True
                    else:
                        eyes_closed_current_frame = False

                if eyes_closed_current_frame:
                    if not alert_cooldown_mgr.is_in_cooldown(self.config["camera_id"], "EYES CLOSED"):
                        self.eye_close_counter += 1
                    else:
                        logging.debug("ğŸ˜´ EYES CLOSED å†·å»ä¸­ï¼Œè·³éç´¯è¨ˆ")
                else:
                    self.eye_close_counter = 0

                # === ç•«å‡ºé—œéµé» ===
                self.draw_keypoints(frame, main_person)

                # === YOLO Cap+Mask æ¨¡å‹æ¨è«– ===
                maskcap_results = self.maskcap_model(frame, verbose=False)
                for r in maskcap_results:
                    if hasattr(r, "boxes") and r.boxes is not None:
                        for box in r.boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            x1, y1, x2, y2 = map(int, box.xyxy[0])

                            if cls_id == 0 and conf >= 0.5:  # cap
                                cap_detected = True
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                                cv2.putText(frame, f"Cap {conf:.2f}", (x1, y1 - 10),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                            elif cls_id == 1 and conf >= 0.5:  # mask
                                mask_detected = True
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)
                                cv2.putText(frame, f"Mask {conf:.2f}", (x1, y1 - 10),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
                            elif cls_id == 4 and conf >= 0.5:  # mouth
                                mouth_detected = True

                # âœ… æ”¹å¯«å¾Œï¼šåªæœ‰ mouth å¯è¦‹ ä¸” mask æœªåµæ¸¬ æ‰ç®—æ²’æˆ´å£ç½©
                if mouth_detected and not mask_detected:
                    if not alert_cooldown_mgr.is_in_cooldown(self.config["camera_id"], "MISSING MASK"):
                        self.missing_mask_count += 1
                else:
                    self.missing_mask_count = 0

                if cap_detected:
                    self.missing_cap_count = 0
                else:
                    if not alert_cooldown_mgr.is_in_cooldown(self.config["camera_id"], "MISSING CAP"):
                        self.missing_cap_count += 1

                # === è­¦å ±æ¢ä»¶ ===
                alert_types = [
                    ("EYES CLOSED", self.eye_close_counter, self.config["close_threshold_frames"]),
                    ("HEAD TURNED", self.head_turn_counter, self.config["head_turn_frames"]),
                    ("MISSING CAP", self.missing_cap_count, self.config["missing_cap_frames"]),
                    ("MISSING MASK", self.missing_mask_count, self.config["missing_mask_frames"]),
                ]

                for alert_type, counter, threshold in alert_types:
                    if counter >= threshold:
                        if not alert_cooldown_mgr.is_in_cooldown(self.config["camera_id"], alert_type):
                            alert_cooldown_mgr.update_last_time(self.config["camera_id"], alert_type)
                            self.reset_counter(alert_type)
                            self.take_screenshot(frame, alert_type)

                # === ç•«é¢æç¤º ===
                if eyes_closed_current_frame:
                    cv2.putText(frame, "EYES CLOSED", (5, 430), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                if head_turned_current_frame:
                    cv2.putText(frame, "HEAD TURNED", (5, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

                cv2.putText(frame, f"Mask Detected: {int(mask_detected)}", (5, 400),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(frame, f"cap Detected: {int(cap_detected)}", (5, 420),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

                if mouth_detected and not mask_detected:
                    cv2.putText(frame, "Mouth+No Mask", (5, 480),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

                logging.debug(
                    f"[{self.config['location']}] EyeCheck: L={l_eye:.4f}, R={r_eye:.4f}, Threshold={self.config['eye_close_threshold']}, Counter={self.eye_close_counter}"
                )    

            except Exception as e:
                logging.error(f"[{self.config['location']}] Frame processing error: {e}")
                traceback_str = traceback.format_exc()
                logging.error(traceback_str)

            time.sleep(0.1)
    
    def draw_keypoints(self, frame, keypoints, confidence_threshold=0.5):
        """åœ¨ç•«é¢ä¸Šç¹ªè£½ YOLO pose é—œéµé»èˆ‡éª¨æ¶é€£ç·š"""
        try:
            # === COCO éª¨æ¶é—œç¯€é€£ç·šå°æ‡‰è¡¨ï¼ˆUltralytics é è¨­é †åºï¼‰ ===
            skeleton_pairs = [
                (5, 7), (7, 9),      # å·¦è‡‚
                (6, 8), (8, 10),     # å³è‡‚
                (11, 13), (13, 15),  # å·¦è…¿
                (12, 14), (14, 16),  # å³è…¿
                (5, 6),              # é›™è‚©
                (11, 12),            # é«–éƒ¨
                (5, 11), (6, 12),    # è»€å¹¹
                (0, 1), (0, 2),      # é¼»å°–åˆ°é›™çœ¼
                (1, 3), (2, 4),      # é›™çœ¼åˆ°é›™è€³
            ]

            # === ç•«å‡ºéª¨æ¶ç·šæ®µ ===
            for i, j in skeleton_pairs:
                if (
                    i < len(keypoints) and j < len(keypoints) and
                    keypoints[i][2] > confidence_threshold and
                    keypoints[j][2] > confidence_threshold
                ):
                    pt1 = (int(keypoints[i][0]), int(keypoints[i][1]))
                    pt2 = (int(keypoints[j][0]), int(keypoints[j][1]))
                    cv2.line(frame, pt1, pt2, (0, 255, 255), 2)

            # === ç•«å‡ºé ­éƒ¨é—œéµé»ï¼ˆå«åç¨±ï¼‰ ===
            head_points = {
                0: ("Nose", (0, 255, 0)),      # é¼»å°– - ç¶ 
                1: ("L_Eye", (255, 0, 0)),     # å·¦çœ¼ - è—
                2: ("R_Eye", (255, 0, 0)),     # å³çœ¼ - è—
                3: ("L_Ear", (0, 255, 255)),   # å·¦è€³ - é»ƒ
                4: ("R_Ear", (0, 255, 255)),   # å³è€³ - é»ƒ
            }

            for idx, (name, color) in head_points.items():
                if idx < len(keypoints) and keypoints[idx][2] > confidence_threshold:
                    x, y = int(keypoints[idx][0]), int(keypoints[idx][1])
                    cv2.circle(frame, (x, y), 3, color, -1)
                    cv2.putText(frame, name, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

        except Exception as e:
            logging.error(f"ç¹ªè£½é—œéµé»æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def display_thread_func(self):
        while self.running:
            disp = self.frame.copy()

            # === æ–°å¢ï¼šé¡¯ç¤ºWORK/SKIPç‹€æ…‹ ===
            current_status = get_current_status()
            current_time = datetime.now().strftime("%H:%M:%S")
            
            # æ ¹æ“šç‹€æ…‹è¨­å®šé¡è‰²
            if current_status == "WORK":
                status_color = (0, 255, 0)  # ç¶ è‰²
                bg_color = (0, 100, 0)      # æ·±ç¶ è‰²èƒŒæ™¯
            else:
                status_color = (0, 255, 255)  # é»ƒè‰²
                bg_color = (0, 100, 100)      # æ·±é»ƒè‰²èƒŒæ™¯
            
            # ç¹ªè£½ç‹€æ…‹èƒŒæ™¯
            cv2.rectangle(disp, (450, 10), (630, 50), bg_color, -1)
            cv2.rectangle(disp, (450, 10), (630, 50), status_color, 2)
            
            # é¡¯ç¤ºç‹€æ…‹æ–‡å­—
            cv2.putText(disp, f"Status: {current_status}", (465, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
            cv2.putText(disp, f"Time: {current_time}", (465, 45), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, status_color, 1)

            # é¡¯ç¤ºä½¿ç”¨çš„è§£ç¢¼æ–¹å¼
            decode_method = "PyAV" if self.use_pyav else "OpenCV"
            cv2.putText(disp, f"Decoder: {decode_method}", (450, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # é¡¯ç¤ºçœ¼éƒ¨è³‡è¨Š
            cv2.putText(disp, f"L Eye: {self.l_eye_value:.3f}  R Eye: {self.r_eye_value:.3f}", 
                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
            
            # é¡¯ç¤ºé ­éƒ¨è½‰å‘è³‡è¨Š
            cv2.putText(disp, f"Head Angle: {self.head_angle:.1f}deg", 
                       (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            cv2.putText(disp, f"Face Asym: {self.face_asymmetry:.3f}", 
                       (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            cv2.putText(disp, f"Side Turn: {self.side_turn_ratio:.2f}", 
                       (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            
            # é¡¯ç¤ºè¨ˆæ•¸å™¨
            cv2.putText(disp, f"Eye Close: {self.eye_close_counter}/{self.config['close_threshold_frames']}", 
                       (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
            cv2.putText(disp, f"Head Turn: {self.head_turn_counter}/{self.config['head_turn_frames']}", 
                       (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)

            cv2.imshow(self.config["location"], disp)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                self.running = False
                break

        cv2.destroyWindow(self.config["location"])

    def take_screenshot(self, annotated_image, alert_type):
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        date_folder = datetime.now().strftime("%Y%m%d")
        unique_id = uuid.uuid4().hex[:6]

        # ğŸ“ å»ºç«‹ capture/YYYYMMDD è·¯å¾‘
        folder_path = os.path.join("capture", date_folder)
        os.makedirs(folder_path, exist_ok=True)

        # === å„²å­˜ä¹¾æ·¨ç•«é¢ï¼ˆæœªç¹ªè£½ï¼‰===
        filename_clean = f"clean_{self.config['camera_id']}_{self.config['location']}_{timestamp}_{unique_id}_{alert_type}.png"
        filepath_clean = os.path.join(folder_path, filename_clean)
        clean_image = self.frame.copy()
        try:
            cv2.imwrite(filepath_clean, clean_image)
        except Exception as e:
            logging.error(f"âŒ å„²å­˜ clean åœ–å¤±æ•—ï¼š{e}")
            return

        # === ä¸å†å„²å­˜ annotated åœ–ï¼Œä½†ä¿ç•™æ–¼è¨˜æ†¶é«”ä¸­ä¾›è½‰æˆ JPG ä¸Šå‚³ ===

        try:
            with logfile_lock:
                with open("FocusDetectionLog.txt", "a") as f:
                    f.write(f"{timestamp}, {self.config['location']}, {filepath_clean}, {alert_type}\n")
        except Exception as e:
            logging.warning(f"âš ï¸ å¯«å…¥æ—¥èªŒå¤±æ•—ï¼š{e}")

        try:
            upload_queue.put((annotated_image.copy(), self.config, alert_type))  # âœ… å‚³ frameï¼Œè€Œä¸æ˜¯ file path
            logging.info(f"ğŸ“· {self.config['location']}ï¼š{alert_type} å·²æˆªåœ–ä¸¦åŠ å…¥ä¸Šå‚³ä½‡åˆ—")
        except Exception as e:
            logging.error(f"âŒ åŠ å…¥ä½‡åˆ—å¤±æ•—: {e}")

def upload_worker():
    valid_result_msgs = {
        "HEAD TURNED": "æª¢æ¸¬åˆ°æŒçºŒè½‰é ­è¡Œç‚ºï¼Œç–‘ä¼¼ä¸å°ˆæ³¨ç‹€æ³",
        "EYES CLOSED": "æª¢æ¸¬åˆ°æŒçºŒé–‰çœ¼è¡Œç‚ºï¼Œç–‘ä¼¼ä¸å°ˆæ³¨ç‹€æ³",
        "MISSING CAP": "ç–‘ä¼¼æœªæˆ´ç„¡å¡µå¸½ï¼Œè«‹åŒä»å„˜é€ŸæŸ¥çœ‹",
        "MISSING MASK": "ç–‘ä¼¼æœªæˆ´å£ç½©ï¼Œè«‹åŒä»å„˜é€ŸæŸ¥çœ‹",
    }

    while True:
        annotated_image, config, alert_type = upload_queue.get()
        try:
            # ğŸ•’ å»ºç«‹æ™‚é–“èˆ‡è·¯å¾‘
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            date_folder = datetime.now().strftime("%Y%m%d")
            unique_id = uuid.uuid4().hex[:6]
            folder_path = os.path.join("capture", date_folder)
            os.makedirs(folder_path, exist_ok=True)

            # ğŸ“¸ å„²å­˜ç‚º JPG
            filename_jpg = f"screenshot_{config['camera_id']}_{config['location']}_{timestamp}_{unique_id}_{alert_type}.jpg"
            jpg_path = os.path.join(folder_path, filename_jpg)
            cv2.imwrite(jpg_path, annotated_image, [int(cv2.IMWRITE_JPEG_QUALITY), 90])

            # âœ… åš´æ ¼æª¢æŸ¥ alert_type æ˜¯å¦åˆè¦
            if alert_type not in valid_result_msgs:
                logging.error(f"â›” ä¸æ˜è­¦å ±é¡å‹ï¼š{alert_type}ï¼Œè«‹æª¢æŸ¥ç¨‹å¼é‚è¼¯èˆ‡ä¾†æºï¼")
                result_msg = f"[éŒ¯èª¤] æœªçŸ¥è­¦å ±é¡å‹ï¼š{alert_type}"
            else:
                result_msg = valid_result_msgs[alert_type]

            # ğŸ“ æº–å‚™ POST åƒæ•¸
            model = {
                "cameraId": config["camera_id"],
                "location": config["location"],
                "eventName": "å°ˆæ³¨åº¦è¾¨è­˜",
                "eventDate": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "notes": alert_type,
                "fileName": os.path.basename(jpg_path),
                "result": result_msg
            }

            # ğŸ“¤ åŸ·è¡Œä¸Šå‚³
            with open(jpg_path, "rb") as f:
                files = {"files": (os.path.basename(jpg_path), f, 'image/jpeg')}
                r = requests.post(config["upload_url"], data=model, files=files, verify=False, timeout=10)

                if r.status_code == 200:
                    logging.info(f"ğŸ“¤ ä¸Šå‚³æˆåŠŸï¼š{config['location']} | {alert_type} | {result_msg}")
                else:
                    logging.warning(f"âš ï¸ ä¸Šå‚³å¤±æ•—ï¼š{config['location']} | HTTP {r.status_code} | {alert_type}")
        except Exception as e:
            logging.error(f"âŒ ä¸Šå‚³éŒ¯èª¤ï¼ˆ{alert_type}ï¼‰ï¼š{e}")
        finally:
            upload_queue.task_done()

def check_camera_signal(rtsp_url, timeout=5):
    """
    å˜—è©¦é–‹å•Ÿ RTSP ä¸²æµç¢ºèªæœ‰ç„¡å½±åƒå›å‚³
    """
    try:
        container = av.open(rtsp_url, timeout=timeout, options={"rtsp_transport": "tcp", "threads": "1"})
        for frame in container.decode(video=0):
            if frame:
                container.close()
                return True
        container.close()
    except Exception as e:
        logging.warning(f"âŒ RTSP é æª¢å¤±æ•—: {rtsp_url} | åŸå› : {e}")
    return False     

def main():
    # å•Ÿå‹•ä¸Šå‚³èƒŒæ™¯åŸ·è¡Œç·’
    upload_thread = threading.Thread(target=upload_worker, daemon=True)
    upload_thread.start()

    camera_monitors = []
    valid_configs = []

    # === RTSP é æª¢éšæ®µ ===
    for idx, cam_config in enumerate(cameras_config):
        rtsp_url = cam_config["rtsp_url"]
        location = cam_config.get("location", f"Cam-{idx}")

        logging.info(f"ğŸ” é æª¢æ”å½±æ©Ÿé€£ç·šï¼š{location}")
        try:
            if check_camera_signal(rtsp_url):
                valid_configs.append(cam_config)
                logging.info(f"âœ… é€šéï¼š{location}")
            else:
                logging.warning(f"ğŸš« ç„¡æ³•é€£ç·šï¼Œç•¥éï¼š{location}")
        except Exception as e:
            logging.error(f"âŒ é æª¢ä¾‹å¤–ï¼š{location} | {e}")

    logging.info(f"ğŸŸ¢ å•Ÿå‹•æˆåŠŸæ”å½±æ©Ÿæ•¸é‡ï¼š{len(valid_configs)} / {len(cameras_config)}")

    if not valid_configs:
        logging.critical("â›” æ²’æœ‰å¯ç”¨çš„æ”å½±æ©Ÿï¼Œç³»çµ±çµæŸ")
        return

    # === åˆå§‹åŒ– CameraMonitor èˆ‡å•Ÿå‹•åŸ·è¡Œç·’ ===
    for idx, cam_config in enumerate(valid_configs):
        monitor = CameraMonitor(cam_config, idx)
        camera_monitors.append(monitor)

        try:
            threading.Thread(target=monitor.read_thread_func, daemon=True, name=f"{cam_config['camera_id']}-reader").start()
            threading.Thread(target=monitor.process_thread_func, daemon=True, name=f"{cam_config['camera_id']}-processor").start()
            # è‹¥éœ€é–‹å•Ÿç•«é¢ï¼š
            #threading.Thread(target=monitor.display_thread_func, daemon=True, name=f"{cam_config['camera_id']}-display").start()
        except Exception as e:
            logging.error(f"âŒ åŸ·è¡Œç·’å•Ÿå‹•å¤±æ•—ï¼š{cam_config['location']} | {e}")

    print(f"ğŸ” å¯¦éš›å•Ÿç”¨ {len(valid_configs)} å°æ”å½±æ©Ÿï¼ˆåŸå§‹è¨­å®šæ•¸: {len(cameras_config)}ï¼‰")
    print("ğŸ“ æ”å½±æ©Ÿè¨­å®šå¾ cameras_config.txt è®€å–")
    print("ğŸ“± ç•«é¢ä¸Šæœƒé¡¯ç¤ºç›®å‰çš„ WORK/SKIP ç‹€æ…‹")
    print("ğŸ”„ è‡ªå‹•åµæ¸¬è§£ç¢¼å•é¡Œä¸¦åˆ‡æ›è‡³ç©©å®šæ¨¡å¼")
    print("ğŸ“Œ æŒ‰ 'q' å¯é—œé–‰ä»»ä¸€è¦–çª—")

    try:
        # ä¸»ç¨‹å¼æŒçºŒé‹è¡Œ + ç›£æ§åŸ·è¡Œç·’ç‹€æ…‹
        while True:
            live_threads = [t.name for t in threading.enumerate() if t.name != "MainThread"]
            logging.debug(f"ğŸ§µ æ´»å‹•ä¸­çš„åŸ·è¡Œç·’ï¼š{len(live_threads)} | {live_threads}")
            time.sleep(10)
    except KeyboardInterrupt:
        print("ğŸ›‘ ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.critical(f"ğŸ’¥ ä¸»åŸ·è¡Œç·’ç•°å¸¸çµ‚æ­¢ï¼š{e}", exc_info=True)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.critical(f"ğŸ’¥ åŸ·è¡ŒéŒ¯èª¤ï¼š{e}", exc_info=True)